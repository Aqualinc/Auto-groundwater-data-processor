---
title: "R Notebook to correct the EQC shallow groundwater network depth sensors for air pressure variations and sensor depth"
output: html_notebook
---


Load additional functions and set directory locations
```{r}

source("\\\\aqualinc-sbs\\data\\ARL Projects\\WL Projects\\WL18036_EQC Earthquake Commission\\R\\Auto-groundwater-data-processor\\GWProcessingFunctions.R")

#Set some directories and filenames
DataDirectory <- "G:\\ARL Projects\\WL Projects\\WL20023_CCC APP\\For ECan April-May 2021\\April-May Download Run\\April-May Data 2021"
OutputDataDirectory <- "G:\\ARL Projects\\WL Projects\\WL20023_CCC APP\\For ECan April-May 2021\\April-May Download Run\\April-May Data 2021\\BaroAndOffsetCorrected"
AutoProcessingDirectory <- "G:\\ARL Projects\\WL Projects\\WL18036_EQC Earthquake Commission\\R\\Auto-groundwater-data-processor"

#For testing on Tims Laptop
#source("D:\\Projects\\Aqualinc\\projects\\APP\\Auto-groundwater-data-processor\\GWProcessingFunctions.R")
#DataDirectory <- "D:\\Projects\\Aqualinc\\projects\\APP\\Data\\Data Harvest 202106"
#OutputDataDirectory <- "D:\\Projects\\Aqualinc\\projects\\APP\\Data\\Data Harvest 202106\\BaroAndOffsetCorrected"
#AutoProcessingDirectory <- "D:\\Projects\\Aqualinc\\projects\\APP\\Auto-groundwater-data-processor"

APPMetadataFile <- file.path(AutoProcessingDirectory,"AppMetadata.csv")
```


Load some helpful libraries
```{r}
if (!require(tools)) install.packages('tools'); library(tools)
if (!require(readxl)) install.packages('readxl'); library(readxl)
if (!require(lubridate)) install.packages('lubridate'); library(lubridate)
```


Create and save the compensated files 
```{r}
#Get the barometric data
BaroData <- BaroDataMerging(DataDirectory)

#Load the metadata file
APPMetaData <- read.csv(APPMetadataFile)

#Get a list of the files to use
#FilesToProcess <- list.files(path = file.path(DataDirectory,"Data Harvest 1"), pattern = '[1-9][0-9,_]*\\.xle$', recursive = TRUE, full.names = TRUE)
#FilesToProcess <- list.files(path = DataDirectory, pattern = '2020.*\\.csv$', recursive = FALSE, full.names = TRUE)
#Annoyingly the filenaming convention changed for the winter 2020 download.
FilesToProcess <- list.files(path = DataDirectory, pattern = 'APP\\d+_\\d{8}\\.csv$', recursive = FALSE, full.names = TRUE)

#FilesToProcess <- FilesToProcess[1:2] #For testing

invisible(lapply(FilesToProcess, function(SingleFile) {

  #Read in the fileafter Checking the file extension
  if (file_ext(SingleFile) == "xle") RawData <- ReadXMLData(SingleFile) 
  if (file_ext(SingleFile) == "csv") RawData <- ReadcsvData(SingleFile)
  
  APPNo <- RawData[['APP']]
  print(APPNo)
  
  #Get the Zone from the APP metadata
  Zone <- APPMetaData$Zone[APPMetaData$APPNo == APPNo]
  
  #Get the sensor offset from the APP metadata
  SensorDepth <- APPMetaData$SensorDepth[APPMetaData$APPNo == APPNo]
  
  #Do the correction for air pressure and sensor depth. Note that I use Zone 0 as this is an invented zone that represents all zones. The ECan barometric data has been set to this zone when their data was formatted to match the standard download format.
  BaroCorrectedData <- BarometricCorrection(RawData[[1]]$'LEVEL',BaroData[['Zone0']],SensorLevelBelowSurface = SensorDepth)
  OutputData <- data.frame(Date = format(index(BaroCorrectedData),"%d/%m/%Y"),
                           Time = format(index(BaroCorrectedData),"%I:%M:%S %p"),
                           LEVEL = coredata(BaroCorrectedData),
                           TEMPERATURE = RawData[['Data']]$TEMPERATURE)
  
  #write this to a new csv file
  StartDate <- format(min(index(BaroCorrectedData)),"%Y%m%d")
  EndDate <- format(max(index(BaroCorrectedData)),"%Y%m%d")
  CombinedFilename <- paste0("APP",formatC(APPNo,width=4,flag="0"),"_",StartDate,"-",EndDate,".csv")
  write.table(OutputData[,c("Date","Time","LEVEL","TEMPERATURE")],file.path(OutputDataDirectory,CombinedFilename),quote=FALSE,row.names = FALSE,sep=",")
  
  return()
}))

```


###Compare manual dips to logged depths
Compare the compensated data to the dipped data. The dipped data is a bit custom, so not really able to make a generic function for it.
There are two dips, one taken during the download, complete with data and time of dip.
The second is the dip from the 2019 download. This doesn't appear to have date and time details. The best that can be done is to assume that it relates to the start of the record for the latest download.
```{r}
LoggerDataDirectory <- "G:\\ARL Projects\\WL Projects\\WL20023_CCC APP\\AllData"
LatestCorrectedDataDirectory <- "G:\\ARL Projects\\WL Projects\\WL20023_CCC APP\\For ECan April-May 2021\\April-May Download Run\\April-May Data 2021\\BaroAndOffsetCorrected"
DippedDataAtEndFile <- "G:\\ARL Projects\\WL Projects\\WL20023_CCC APP\\For ECan April-May 2021\\Location-ordered April2021 APP Download Fieldsheet.xlsx"
DippedDataAtStartFile <- "G:\\ARL Projects\\WL Projects\\WL20023_CCC APP\\Data_Winter_2020\\Master Spreadsheet\\Location-ordered July2020 APP Download Fieldsheet.xlsx"   
OutputFileName <- "G:\\ARL Projects\\WL Projects\\WL20023_CCC APP\\For ECan April-May 2021\\DippedQC.csv"

#Test directories on Tim's laptop
#LoggerDataDirectory <- "D:\\Projects\\Aqualinc\\projects\\APP\\Data\\AllData"
#LatestCorrectedDataDirectory <- "D:\\Projects\\Aqualinc\\projects\\APP\\Data\\Data Harvest 202106\\BaroAndOffsetCorrected"
#DippedDataAtEndFile <- "D:\\Projects\\Aqualinc\\projects\\APP\\Auto-groundwater-data-processor\\Location-ordered April2021 APP Download Fieldsheet.xlsx"
#DippedDataAtStartFile <- "D:\\Projects\\Aqualinc\\projects\\APP\\Auto-groundwater-data-processor\\Location-ordered July2020 APP Download Fieldsheet.xlsx"
#OutputFileName <- "D:\\Projects\\Aqualinc\\projects\\APP\\Auto-groundwater-data-processor\\DippedQC.csv"


LoggerDataFiles <- list.files(LoggerDataDirectory, full.names = TRUE)

#Get the dipped data taken at the previous download (i.e. at the beginning of the data for the current download)

{
  DippedStartData <- read_xlsx(DippedDataAtStartFile, sheet = "Field use", range= cell_cols("A:L"),col_types = c("numeric",rep("skip",8),"date","text","numeric"))
  
  #Remove any lines without a Time, assumed that they were not downloaded
  DippedStartData <- DippedStartData[!is.na(DippedStartData$Time),]
  
  #the times are read in as characters, even though they should be a decimal number representing a fraction of a day).
  #To create an R Date-time, need to, add the times to the dates. The times needed to be converted from fractions of a day to seconds
  DippedStartData$DateTimes <- DippedStartData$Date + as.numeric(DippedStartData$'Time') * 3600 * 24
  
  #Explicitly force the time zone to NZST
  DippedStartData$DateTimes <- force_tz(DippedStartData$DateTimes, "Etc/GMT-12")
}
  
#   #Remove any lines not related to an APP number (assumed blank lines)
#   DippedStartData <- DippedStartData[!is.na(DippedStartData$`APP No.`),]
#   
# #Explicitly force the ingested timezone to be NZST, overriding the read_xls default of UTC
# DippedStartData$Date <- force_tz(DippedStartData$Date, "Etc/GMT-12")
# 
# #When importing, the times that were recognised as times were converted to an excel time object (i.e. a decimal number representing a fraction of a day). All the others were left as text. Because all the values in a column within R need to be one "type" they are all conveted to "character" on import. This makes a mess!
# 
# #Of the time strings that still need converting, the format varies, so they need to be standardised.
# Times <- DippedStartData$Time
# 
# #convert all the "am" and "pm" variants to  "AM" and "PM"
# Times <- sub(" a.m"," AM",Times)
# Times <- sub(" p.m"," PM",Times)
# Times <- sub("a.m."," AM",Times)
# Times <- sub("p.m."," PM",Times)
# Times <- sub("a.m"," AM",Times)
# Times <- sub("p.m"," PM",Times)
# Times <- sub("A.M.","AM",Times)
# Times <- sub("P.M.","PM",Times)
# 
# #Now that all they can be converted into a time object, but only do the entries that don't look numeric.
# #To allign with the excel numeric format, convert from seconds to fractions of a day, and make the origin the POSIXct origin (1970-01-01)
# Times[which(is.na(as.numeric(Times)))] <-
# as.character(as.numeric(parse_date_time(paste("1970-01-01", Times[which(is.na(as.numeric(Times)))]),"%Y-%m-%d %I:%M:%S %p"))/(60*60*24))
# 
# #Can now convert them
# #Times5 <- as.POSIXct(3600 * as.numeric(Times), origin = "1970-01-01", tz="Etc/GMT-12")
# 
# Times5 <- as.numeric(Times)
# 
# #And add them to the dates
# DippedStartData$DateTimes <- DippedStartData$Date + Times5 * 3600 * 24
# }


#Get the dipped data taken during the download (i.e. the latest)
{
  DippedEndData <- read_xlsx(DippedDataAtEndFile, sheet = "Print out", range= cell_cols("A:F"),col_types = c("numeric",rep("skip",2),"date","text","numeric"))
  
  #Remove any lines without a time, assumed that they were not downloaded
  DippedEndData <- DippedEndData[!is.na(DippedEndData$'Time(Autumn 2021'),]
  
  #the times are read in as characters, even though they should be a decimal number representing a fraction of a day).
  #To create an R Date-time, need to, add the times to the dates. The times needed to be converted from fractions of a day to seconds
  DippedEndData$DateTimes <- DippedEndData$'Date (Autumn 2021)' + as.numeric(DippedEndData$'Time(Autumn 2021') * 3600 * 24
  
  #Explicitly force the time zone to NZST
  DippedEndData$DateTimes <- force_tz(DippedEndData$DateTimes, "Etc/GMT-12")
}


#Check that all downloaded sites have a reading in the Field note spreadsheet.
AllDownloadedSites <- list.files(path = LatestCorrectedDataDirectory, pattern = 'APP\\d+_\\d{8}.*\\.csv$', recursive = TRUE, full.names = FALSE)
AllDownloadedSiteAPPNumbers <- as.numeric(sub(pattern="^APP([0-9]*)_.*csv","\\1",x=AllDownloadedSites))
FieldNoteSites <- DippedEndData$`APP No.`[which(!is.na(DippedEndData$'Time(Autumn 2021'))]
print(paste("APP sites missing in field note spreadsheet:",AllDownloadedSiteAPPNumbers[which(!AllDownloadedSiteAPPNumbers %in% FieldNoteSites)]))

CompleteDataFileSites <- as.numeric(sub(pattern="^APP([0-9]*)_.*csv","\\1",x=basename(LoggerDataFiles)))
print(paste("APP sites missing from the complete data holdings:",AllDownloadedSiteAPPNumbers[which(!AllDownloadedSiteAPPNumbers %in% CompleteDataFileSites)]))

#Work through each APP number in turn, get the dipped data and the time of measurement and get the associated logged data, then compare the two
   
 DippedQC <- lapply(seq_along(AllDownloadedSiteAPPNumbers), function(Index) {   #for testing Index <- 45
  APPNo       <- AllDownloadedSiteAPPNumbers[Index]
  print(APPNo)
  DippedEndDataIndex <- which(DippedEndData$`APP No.`== APPNo)
  
  #Get the related logger data
    LoggerDataFilesIndices <- which(startsWith(basename(LoggerDataFiles), sprintf("APP%04.0f", APPNo)))
  
  #If there is only one file then open it
  if(length(LoggerDataFilesIndices)==1) {
    LoggedDataRaw <- read.table(LoggerDataFiles[LoggerDataFilesIndices],header=TRUE,sep=",",stringsAsFactors = FALSE)
  } else if(length(LoggerDataFilesIndices)>1) {
    #If there is more than one, then open them both and combine them
    LoggedDataRaw <- read.table(LoggerDataFiles[LoggerDataFilesIndices[1]],header=TRUE,sep=",",stringsAsFactors = FALSE)
    for(FileIndex in LoggerDataFilesIndices[-1]) {
      LoggedDataRaw <- rbind(LoggedDataRaw,read.table(LoggerDataFiles[FileIndex],header=TRUE,sep=",",stringsAsFactors = FALSE))
    }} else {
      #If there are none, then output null
      LoggedDataRaw <- NULL 
    }
  LoggedDataRaw$Time <- sub("a.m.","AM",LoggedDataRaw$Time)
  LoggedDataRaw$Time <- sub("p.m.","PM",LoggedDataRaw$Time)
  LoggedDataDates    <- as.POSIXct(paste(LoggedDataRaw$Date,LoggedDataRaw$Time),format="%d/%m/%Y %H:%M:%S",tz="Etc/GMT-12")

  #Get the end dips
  DippedEndDepth <- DippedEndData$`Dipped Water Level (Autumn 2021)`[DippedEndDataIndex]

  if(!is.na(DippedEndDepth)){
  DippedEndDate  <- DippedEndData$DateTimes[DippedEndDataIndex]
  if(length(LoggedDataDates) > 0){
  IndexOfClosestLoggerEndObservation <- which(abs(LoggedDataDates-DippedEndDate) == min(abs(LoggedDataDates - DippedEndDate),na.rm = TRUE))[1]
  LoggedEndDepth        <- LoggedDataRaw$LEVEL[IndexOfClosestLoggerEndObservation] * -1 #multiply by negative 1 to convert to depth, rather than height above measuring point
  } else {
    IndexOfClosestLoggerEndObservation <- NA
    LoggedEndDepth <- NA
  }
    
  LoggedEndDateTime     <- LoggedDataDates[IndexOfClosestLoggerEndObservation]
  DepthEndDifference    <- DippedEndDepth - LoggedEndDepth
  TimeEndDifference     <- as.numeric(abs(difftime(DippedEndDate,LoggedEndDateTime,units = "mins")))
} else { #Case of no end dip data
  DippedEndDate      <- NA
  LoggedEndDateTime  <- NA
  LoggedEndDepth     <- NA
  TimeEndDifference  <- NA
  DepthEndDifference <- NA
}
  
  #Get the start dips
    DippedStartDepth <- as.numeric(DippedStartData$`Dipped Water Level (m) Winter 2020`[as.numeric(DippedStartData$`APP No.`)== APPNo])
    if (length(DippedStartDepth) > 0){
      if(!is.na(DippedStartDepth)){
      DippedStartDate  <- DippedStartData$DateTimes[as.numeric(DippedStartData$`APP No.`)== APPNo]
      if(length(LoggedDataDates) > 0){
        IndexOfClosestLoggerStartObservation <- which(abs(LoggedDataDates-DippedStartDate) == min(abs(LoggedDataDates - DippedStartDate),na.rm=TRUE))[1]
        LoggedStartDepth        <- LoggedDataRaw$LEVEL[IndexOfClosestLoggerStartObservation] * -1 #multiply by negative 1 to convert to depth, rather than height above measuring point
        } else {
        IndexOfClosestLoggerStartObservation <- NA
        LoggedStartDepth <- NA
      }
        
    LoggedStartDateTime     <- LoggedDataDates[IndexOfClosestLoggerStartObservation]
    DepthStartDifference    <- DippedStartDepth - LoggedStartDepth
    TimeStartDifference     <- as.numeric(abs(difftime(DippedStartDate,LoggedStartDateTime,units = "mins")))
    }} else{
  DippedStartDate      <- NA
  LoggedStartDateTime  <- NA
  LoggedStartDepth     <- NA
  TimeStartDifference  <- NA
  DepthStartDifference <- NA
    }
    

    
  ChangeInDipOffset     <- DepthEndDifference - DepthStartDifference
    Output             <- data.frame(APPNo = APPNo,
                                     'Start Dip Date-Time' =  DippedStartDate, 'Start Dip Depth(m)' = DippedStartDepth,
                                     'Start Dip Loggers Nearest Date-Time' = LoggedStartDateTime, 'Start Dip Logger Depth (m)' = LoggedStartDepth,
                                     'Start Dip Logger Time Difference (minutes)'= TimeStartDifference, 'Start Dip Logger Depth Difference (m)' = DepthStartDifference,
                                     'End Dip Date-Time' =  DippedEndDate, 'End Dip Depth (m)' = DippedEndDepth,
                                     'End Dip Loggers Nearest Date-Time' = LoggedEndDateTime, 'End Dip Logger Depth (m)' = LoggedEndDepth,
                                     'End Dip Logger Time Difference (minutes)'= TimeEndDifference, 'End Dip Depth Difference (m)' = DepthEndDifference,
                                     "Change in dip difference (m)" = ChangeInDipOffset,check.names=FALSE)
  return(Output)
  })
DipToAutoComparisonTable <- do.call(rbind,DippedQC)
write.table(DipToAutoComparisonTable,OutputFileName,quote=FALSE,row.names = FALSE,sep=",")
```

Need to check whether an offset was introduced because of the downloading
Get the first date of the latest download, check the logged depth for that time and the time immediately before it
Identify any loggers with an offset of more than 10 cm.
```{r}
AllDataDirectory <- "G:\\ARL Projects\\WL Projects\\WL20023_CCC APP\\AllData"
LatestDataDirectory <- "G:\\ARL Projects\\WL Projects\\WL20023_CCC APP\\For ECan April-May 2021\\April-May Download Run\\April-May Data 2021\\BaroAndOffsetCorrected"
OutputStepChangeFile <- "G:\\ARL Projects\\WL Projects\\WL20023_CCC APP\\For ECan April-May 2021\\StepChangeOnDownloadQC.csv"

#Test directories on Tim's laptop
#AllDataDirectory <- "D:\\Projects\\Aqualinc\\projects\\APP\\Data\\AllData" 
#LatestDataDirectory <- "D:\\Projects\\Aqualinc\\projects\\APP\\Data\\Data Harvest 202009\\BaroAndOffsetCorrected"
#OutputStepChangeFile <- "D:\\Projects\\Aqualinc\\projects\\APP\\Data\\StepChangeOnDownloadQC.csv"

#Get all the file names from the latest data download
LatestDataFiles <- list.files(LatestDataDirectory, full.names = TRUE)

#Work through each file in turn
OffsetDetection <- lapply(seq_along(LatestDataFiles), function(CurrentFileIndex) {
  CurrentFileName <- LatestDataFiles[CurrentFileIndex]
  APPNo <- as.numeric(sub("^APP([0-9]*)_.*","\\1",basename(CurrentFileName)))
  print(APPNo)
  #Read the first data line from the file and get the first date time
  FirstLine <- readLines(CurrentFileName,2,)[2]
  #Get the date and time of the first line
  DateTimeText <- paste(strsplit(FirstLine,",")[[1]][1:2],collapse=" ")
  
  #Now go through the ridiculous process of getting the a.m./p.m. correct
  DateTimeText <- sub("a.m.","am",DateTimeText)
  DateTimeText <- sub("p.m.","pm",DateTimeText)
  
  DateTime <- as.POSIXct(DateTimeText, format = "%d/%m/%Y %I:%M:%S %p",tz = "Etc/GMT-12")
  
  #Now find the index of this date in the complete data record, and have a look at the level for the time, and the previous time
  #Start by opening the correct file. Assume there is only one for each APP number
  CompleteDataFileOfInterest <- list.files(AllDataDirectory,pattern = sprintf("^APP%04.0f_.*",APPNo), full.names = TRUE)[1]
  CompleteData <- read.table(CompleteDataFileOfInterest,sep=",",stringsAsFactors = FALSE,header=TRUE)
  CompleteDateTime <- as.POSIXct(paste(CompleteData$Date,CompleteData$Time),format="%d/%m/%Y %H:%M:%S",tz= "Etc/GMT-12")
  IndexOfStartofMostRecentDataDownload <- which.min(abs(CompleteDateTime - DateTime))
  DeltaLevel <- diff(CompleteData$LEVEL[c((IndexOfStartofMostRecentDataDownload-1),(IndexOfStartofMostRecentDataDownload+1))])
  LevelChangeSummary <- data.frame(APPNo = APPNo, FirstNewLoggedLevel =CompleteData$LEVEL[IndexOfStartofMostRecentDataDownload], FirstNewLoggedDateTime =CompleteDateTime[IndexOfStartofMostRecentDataDownload],
             SecondNewLoggedLevel=CompleteData$LEVEL[IndexOfStartofMostRecentDataDownload + 1],SecondNewLoggedDateTime=CompleteDateTime[IndexOfStartofMostRecentDataDownload + 1],
             LastOldLoggedLevel =CompleteData$LEVEL[IndexOfStartofMostRecentDataDownload - 1], LastOldLoggedDateTime=CompleteDateTime[IndexOfStartofMostRecentDataDownload - 1],
             LevelDifference = DeltaLevel)
  return(LevelChangeSummary)
})

StepChangeOnDownloadQCTable <- do.call(rbind,OffsetDetection)
write.table(StepChangeOnDownloadQCTable,OutputStepChangeFile,quote=FALSE,row.names = FALSE,sep=",")
```

