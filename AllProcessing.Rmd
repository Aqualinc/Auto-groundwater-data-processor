---
title: "R Notebook to correct the EQC shallow groundwater network depth sensors for air pressure variations and sensor depth"
output: html_notebook
---


Use the first data harvest as an example
```{r}
#Load my functions and any libraries that I may need
#source("\\\\aqualinc-sbs\\data\\ARL Projects\\WL Projects\\WL18036_EQC Earthquake Commission\\R\\Auto-groundwater-data-processor\\GWProcessingFunctions.R")
source("D:\\Projects\\Aqualinc\\projects\\APP\\Auto-groundwater-data-processor\\GWProcessingFunctions.R")

#Set some directories and filenames
#DataDirectory <- "G:\\ARL Projects\\WL Projects\\WL18036_EQC Earthquake Commission\\Data\\HighResolutionData\\FromT_T_January2019\\DH1 - DH4\\DH1-DH4 Raw"
DataDirectory <- "H:\\WL Projects\\WL20023_CCC APP\\Data 2020"

#OutputDataDirectory <- "G:\\ARL Projects\\WL Projects\\WL18036_EQC Earthquake Commission\\R\\Auto-groundwater-data-processor\\OutputData"
OutputDataDirectory <- "H:\\WL Projects\\WL20023_CCC APP\\Data 2020\\BaroAndOffsetCorrected"

#AutoProcessingDirectory <- "G:\\ARL Projects\\WL Projects\\WL18036_EQC Earthquake Commission\\R\\Auto-groundwater-data-processor"
AutoProcessingDirectory <- "C:\\Users\\Owner\\Documents\\Projects\\Aqualinc\\APP\\Auto-groundwater-data-processor"

APPMetadataFile <- file.path(AutoProcessingDirectory,"AppMetadata.csv")
```


Load some helpful libraries
```{r}
if (!require(tools)) install.packages('tools'); library(tools)
if (!require(readxl)) install.packages('readxl'); library(readxl)
if (!require(lubridate)) install.packages('lubridate'); library(lubridate)
```


Create and save the compensated files 
```{r}
#Get the barometric data
BaroData <- BaroDataMerging(DataDirectory)

#Load the metadata file
APPMetaData <- read.csv(APPMetadataFile)

#Get a list of the files to use
#FilesToProcess <- list.files(path = file.path(DataDirectory,"Data Harvest 1"), pattern = '[1-9][0-9,_]*\\.xle$', recursive = TRUE, full.names = TRUE)
FilesToProcess <- list.files(path = DataDirectory, pattern = '2020.*\\.csv$', recursive = FALSE, full.names = TRUE)

FilesToProcess <- FilesToProcess[1:2]

invisible(lapply(FilesToProcess, function(SingleFile) {

  #Read in the fileafter Checking the file extension
  if (file_ext(SingleFile) == "xle") RawData <- ReadXMLData(SingleFile) 
  if (file_ext(SingleFile) == "csv") RawData <- ReadcsvData(SingleFile)
  
  APPNo <- RawData[['APP']]
  print(APPNo)
  
  #Get the Zone from the APP metadata
  Zone <- APPMetaData$Zone[APPMetaData$APPNo == APPNo]
  
  #Get the sensor offset from the APP metadata
  SensorDepth <- APPMetaData$SensorDepth[APPMetaData$APPNo == APPNo]
  
  #Do the correction for air pressure and sensor depth. Note that I use Zone 0 as this is an invented zone that represents all zones. The ECan barometric data has been set to this zone when their data was formatted to match the standard download format.
  BaroCorrectedData <- BarometricCorrection(RawData[[1]]$'LEVEL',BaroData[['Zone0']],SensorLevelBelowSurface = SensorDepth)
  
  OutputData <- data.frame(Date = format(index(BaroCorrectedData),"%d/%m/%Y"),
                           Time = format(index(BaroCorrectedData),"%I:%M:%S %p"),
                           LEVEL = coredata(BaroCorrectedData),
                           TEMPERATURE = RawData[['Data']]$TEMPERATURE)
  
  #write this to a new csv file
  StartDate <- format(min(index(BaroCorrectedData)),"%Y%m%d")
  EndDate <- format(max(index(BaroCorrectedData)),"%Y%m%d")
  CombinedFilename <- paste0("APP",formatC(APPNo,width=4,flag="0"),"_",StartDate,"-",EndDate,".csv")
  write.table(OutputData[,c("Date","Time","LEVEL","TEMPERATURE")],file.path(OutputDataDirectory,CombinedFilename),quote=FALSE,row.names = FALSE,sep=",")
  
  return()
}))

```


###Compare manual dips to logged depths
Compare the compensated data to the dipped data. The dipped data is a bit custom, so not really able to make a generic function for it.
There are two dips, one taken during the download, complete with data and time of dip.
The second is the dip from the 2019 download. This doesn't appear to have date and time details. The best that can be done is to assume that it relates to the start of the record for the latest download.
```{r}
LoggerDataDirectory <- "H:\\WL Projects\\WL20023_CCC APP\\Data 2020\\AllData"
DippedDataAtEndFile <- "H:\\WL Projects\\WL20023_CCC APP\\Data 2020\\Monitoring Spreadsheet\\20200221.Piezometer Downloads_TIM K.xlsx" #Aqualinc servers via Tims laptop

DippedDataAtEndFile <- "D:\\Projects\\Aqualinc\\projects\\APP\\Auto-groundwater-data-processor\\20200221.Piezometer Downloads_TIM K.xlsx" #Copy on Tim's laptop

DippedDataAtStartFile <- "H:\\WL Projects\\WL20023_CCC APP\\T&T Stocktake 2019\\ECan EQC stocktake\\Aqualinc\\Aug2019.Piezometer Stocktake_Rev1.1.xlsx"   #Aqualinc servers via Tims laptop
DippedDataAtStartFile <- "D:\\Projects\\Aqualinc\\projects\\APP\\Auto-groundwater-data-processor\\Aug2019.Piezometer Stocktake_Rev1.1.xlsx"     #On Tims Laptop

LoggerDataFiles <- list.files(LoggerDataDirectory, full.names = TRUE)

#Get the dipped data taken during the download (i.e. the latest)
{
DippedEndData <- read_xlsx(DippedDataAtEndFile, sheet = "TIM K", range= cell_cols("B:L"))
  
  #Remove any lines not related to an APP number (assumed blank lines)
  DippedEndData <- DippedEndData[!is.na(DippedEndData$`APP No.`),]
  
#Explicitly force the ingested timezone to be NZST, overriding the read_xls default of UTC
DippedEndData$Date <- force_tz(DippedEndData$Date, "Etc/GMT-12")

#When importing, the times that were recognised as times were converted to an excel time object (i.e. a decimal number representing a fraction of a day). All the others were left as text. Because all the values in a column within R need to be one "type" they are all conveted to "character" on import. This makes a mess!

#Of the time strings that still need converting, the format varies, so they need to be standardised.
Times <- DippedEndData$Time

#convert all the "am" and "pm" variants to  "AM" and "PM"
Times <- sub(" a.m"," AM",Times)
Times <- sub(" p.m"," PM",Times)
Times <- sub("a.m."," AM",Times)
Times <- sub("p.m."," PM",Times)
Times <- sub("a.m"," AM",Times)
Times <- sub("p.m"," PM",Times)
Times <- sub("A.M.","AM",Times)
Times <- sub("P.M.","PM",Times)

#Now that all they can be converted into a time object, but only do the entries that don't look numeric.
#To allign with the excel numeric format, convert from seconds to fractions of a day, and make the origin the POSIXct origin (1970-01-01)
Times[which(is.na(as.numeric(Times)))] <-
as.character(as.numeric(parse_date_time(paste("1970-01-01", Times[which(is.na(as.numeric(Times)))]),"%Y-%m-%d %I:%M:%S %p"))/(60*60*24))

#Can now convert them
#Times5 <- as.POSIXct(3600 * as.numeric(Times), origin = "1970-01-01", tz="Etc/GMT-12")

Times5 <- as.numeric(Times)

#And add them to the dates
DippedEndData$DateTimes <- DippedEndData$Date + Times5 * 3600 * 24
}


#Get the dipped data taken at the previous download (i.e. at the begining of the data for thie current download)
{
  DippedStartData <- read_xlsx(DippedDataAtStartFile, sheet = "APP", range= cell_cols("A:K"))
  
  #Remove any lines not related to an APP number (assumed blank lines)
  DippedStartData <- DippedStartData[!is.na(DippedStartData$`APP No.`),]
  
  #The dates are read in as character, even though they should be numbers, so need to be converted to POSIXct but with an excel origin
  #Care is needed to get the time zone correct.
  DateRead <- as.POSIXct(as.numeric(DippedStartData$'Date Read')*60*60*24,origin=as.POSIXct("1899-12-30 00:00:00", tz= "Etc/GMT-12"),tz="Etc/GMT-12")

#the times are read in as characters, even though they should be a decimal number representing a fraction of a day).
#To create an R Date-time, need to, add the times to the dates. The times needed to be converted from fractions of a day to seconds
  DippedStartData$DateTimes <- DateRead + as.numeric(DippedStartData$'Time Read') * 3600 * 24
}


#Work through each APP number in turn, get the dipped data and the time of measurement and get the associated logged data, then compare the two
DippedQC <- lapply(seq_along(DippedEndData$`APP No.`), function(Index) {   
 #DippedQC <- lapply(seq_along(DippedEndData$`APP No.`[1:5]), function(Index) {   #for testing Index <- 209
  APPNo       <- DippedEndData$`APP No.`[Index]
  print(APPNo)
  
  #Get the related logger data
    LoggerDataFilesIndices <- which(startsWith(basename(LoggerDataFiles), sprintf("APP%04.0f", APPNo)))
  
  #If there is only one file then open it
  if(length(LoggerDataFilesIndices)==1) {
    LoggedDataRaw <- read.table(LoggerDataFiles[LoggerDataFilesIndices],header=TRUE,sep=",",stringsAsFactors = FALSE)
  } else if(length(LoggerDataFilesIndices)>1) {
    #If there is more than one, then open them both and combine them
    LoggedDataRaw <- read.table(LoggerDataFiles[LoggerDataFilesIndices[1]],header=TRUE,sep=",",stringsAsFactors = FALSE)
    for(FileIndex in LoggerDataFilesIndices[-1]) {
      LoggedDataRaw <- rbind(LoggedDataRaw,read.table(LoggerDataFiles[FileIndex],header=TRUE,sep=",",stringsAsFactors = FALSE))
    }} else {
      #If there are none, then output null
      LoggedDataRaw <- NULL 
    }
  LoggedDataRaw$Time <- sub("a.m.","AM",LoggedDataRaw$Time)
  LoggedDataRaw$Time <- sub("p.m.","PM",LoggedDataRaw$Time)
  LoggedDataDates    <- as.POSIXct(paste(LoggedDataRaw$Date,LoggedDataRaw$Time),format="%d/%m/%Y %H:%M:%S",tz="Etc/GMT-12")

  #Get the end dips
  DIppedEndDepth <- DippedEndData$`Dipped water level 2020 (m)`[Index]

  if(!is.na(DIppedEndDepth)){
  DippedEndDate  <- DippedEndData$DateTimes[Index]
  if(length(LoggedDataDates) > 0){
  IndexOfClosestLoggerEndObservation <- which(abs(LoggedDataDates-DippedEndDate) == min(abs(LoggedDataDates - DippedEndDate),na.rm = TRUE))[1]
  LoggedEndDepth        <- LoggedDataRaw$LEVEL[IndexOfClosestLoggerEndObservation] * -1 #multiply by negative 1 to convert to depth, rather than height above measuring point
  } else {
    IndexOfClosestLoggerEndObservation <- NA
    LoggedEndDepth <- NA
  }
    
  LoggedEndDateTime     <- LoggedDataDates[IndexOfClosestLoggerEndObservation]
  DepthEndDifference    <- DIppedEndDepth - LoggedEndDepth
  TimeEndDifference     <- as.numeric(abs(difftime(DippedEndDate,LoggedEndDateTime,units = "mins")))
} else { #Case of no end dip data
  DippedEndDate      <- NA
  LoggedEndDateTime  <- NA
  LoggedEndDepth     <- NA
  TimeEndDifference  <- NA
  DepthEndDifference <- NA
}
  
  #Get the start dips
    DippedStartDepth <- as.numeric(DippedStartData$`Dipped water level 2019 (m)`[as.numeric(DippedStartData$`APP No.`)== APPNo])
    if(!is.na(DippedStartDepth)){
    DippedStartDate  <- DippedStartData$DateTimes[as.numeric(DippedStartData$`APP No.`)== APPNo]
    if(length(LoggedDataDates) > 0){
      IndexOfClosestLoggerStartObservation <- which(abs(LoggedDataDates-DippedStartDate) == min(abs(LoggedDataDates - DippedStartDate),na.rm=TRUE))[1]
      LoggedStartDepth        <- LoggedDataRaw$LEVEL[IndexOfClosestLoggerStartObservation] * -1 #multiply by negative 1 to convert to depth, rather than height above measuring point
      } else {
      IndexOfClosestLoggerStartObservation <- NA
      LoggedStartDepth <- NA
    }
      
  LoggedStartDateTime     <- LoggedDataDates[IndexOfClosestLoggerStartObservation]
  DepthStartDifference    <- DippedStartDepth - LoggedStartDepth
  TimeStartDifference     <- as.numeric(abs(difftime(DippedStartDate,LoggedStartDateTime,units = "mins")))
    } else{
  DippedStartDate      <- NA
  LoggedStartDateTime  <- NA
  LoggedStartDepth     <- NA
  TimeStartDifference  <- NA
  DepthStartDifference <- NA
    }
    

    
  ChangeInDipOffset     <- DepthEndDifference - DepthStartDifference
    Output             <- data.frame(APPNo = APPNo,
                                     'Start Dip Date-Time' =  DippedStartDate, 'Start Dip Depth(m)' = DippedStartDepth,
                                     'Start Dip Loggers Nearest Date-Time' = LoggedStartDateTime, 'Start Dip Logger Depth (m)' = LoggedStartDepth,
                                     'Start Dip Logger Time Difference (minutes)'= TimeStartDifference, 'Start Dip Logger Depth Difference (m)' = DepthStartDifference,
                                     'End Dip Date-Time' =  DippedEndDate, 'End Dip Depth (m)' = DIppedEndDepth,
                                     'End Dip Loggers Nearest Date-Time' = LoggedEndDateTime, 'End Dip Logger Depth (m)' = LoggedEndDepth,
                                     'End Dip Logger Time Difference (minutes)'= TimeEndDifference, 'End Dip Depth Difference (m)' = DepthEndDifference,
                                     "Change in dip difference (m)" = ChangeInDipOffset,check.names=FALSE)
  return(Output)
  })
DipToAutoComparisonTable <- do.call(rbind,DippedQC)
write.table(DipToAutoComparisonTable,"H:\\WL Projects\\WL20023_CCC APP\\Data 2020\\Monitoring Spreadsheet\\DippedQC.csv",quote=FALSE,row.names = FALSE,sep=",")
```

Need to check whether an offset was introduced because of the downloading
Get the first date of the latest download, check the logged depth for that time and the time immediately before it
Identify any loggers with an offset of more than 10 cm.
```{r}
AllDataDirectory <- "H:\\WL Projects\\WL20023_CCC APP\\Data 2020\\AllData"
LatestDataDirectory <- "H:\\WL Projects\\WL20023_CCC APP\\Data 2020\\BaroAndOffsetCorrected"

#Get all the filenames from the latest data download
LatestDataFiles <- list.files(LatestDataDirectory, full.names = TRUE)

#Work through each file in turn
OffsetDetection <- lapply(seq_along(LatestDataFiles), function(CurrentFileIndex) {
  CurrentFileName <- LatestDataFiles[CurrentFileIndex]
  APPNo <- as.numeric(sub("^APP([0-9]*)_.*","\\1",basename(CurrentFileName)))
  #Read the first data line from the file and get the first date time
  FirstLine <- readLines(CurrentFileName,2,)[2]
  #Get the date and time of the first line
  DateTimeText <- paste(strsplit(FirstLine,",")[[1]][1:2],collapse=" ")
  
  #Now go through the rediculous process of getting the a.m./p.m. correct
    DateTimeText <- sub("a.m.","am",DateTimeText)
  DateTimeText <- sub("p.m.","pm",DateTimeText)
  
  DateTime <- as.POSIXct(DateTimeText, format = "%d/%m/%Y %I:%M:%S %p",tz = "Etc/GMT-12")
  
  #Now find the index of this date in the complete data record, and have a look at the level for the time, and the previous time
  #Start by opening the correct file. Assume there is only one for each APP number
  CompleteDataFileOfInterest <- list.files(AllDataDirectory,pattern = sprintf("^APP%04.0f_.*",APPNo), full.names = TRUE)[1]
  CompleteData <- read.table(CompleteDataFileOfInterest,sep=",",stringsAsFactors = FALSE,header=TRUE)
  CompleteDateTime <- as.POSIXct(paste(CompleteData$Date,CompleteData$Time),format="%d/%m/%Y %H:%M:%S",tz= "Etc/GMT-12")
  IndexOfStartofMostRecentDataDownload <- which.min(abs(CompleteDateTime - DateTime))
  DeltaLevel <- diff(CompleteData$LEVEL[c((IndexOfStartofMostRecentDataDownload-1),(IndexOfStartofMostRecentDataDownload+1))])
  LevelChangeSummary <- data.frame(FirstNewLoggedLevel =CompleteData$LEVEL[IndexOfStartofMostRecentDataDownload], FirstNewLoggedDateTime =CompleteDateTime[IndexOfStartofMostRecentDataDownload],
             SecondNewLoggedLevel=CompleteData$LEVEL[IndexOfStartofMostRecentDataDownload + 1],SecondNewLoggedDateTime=CompleteDateTime[IndexOfStartofMostRecentDataDownload + 1],
             LastOldLoggedLevel =CompleteData$LEVEL[IndexOfStartofMostRecentDataDownload - 1], LastOldLoggedDateTime=CompleteDateTime[IndexOfStartofMostRecentDataDownload - 1],
             LevelDifference = DeltaLevel)
  return(LevelChangeSummary)
})

StepChangeOnDownloadQCTable <- do.call(rbind,OffsetDetection)
write.table(StepChangeOnDownloadQCTable,"H:\\WL Projects\\WL20023_CCC APP\\Data 2020\\Monitoring Spreadsheet\\StepChangeOnDownloadQC.csv",quote=FALSE,row.names = FALSE,sep=",")
```

